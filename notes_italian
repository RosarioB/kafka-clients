NOTE KAFKA

Il docker compose completo con tutti i servizi di kafka/confluent l'abbiamo preso qui:
    https://github.com/confluentinc/cp-all-in-one/blob/7.4.0-post/cp-all-in-one/docker-compose.yml

cd /Users/rosarioborgesi/Documents/workspaces/kafka-prova/docker

docker-compose -f kafka-docker-compose.yml ps --> per vedere se i container sono up

docker stats  --> per vedere l'uso di memoria e cpu dei container

docker exec -it zookeeper bash --> per aprire la shell di bash nel container di zookeeper
docker exec -it broker bash   --| per aprire la shell bash del container broker

http://localhost:9021/ --> URL della UI del control center

***************** TEST PER VERIFICARE SE IL BROKER E' PRESENTE SU ZOOKEEPER *****************
    docker exec -it zookeeper bash -
    zookeeper-shell zookeeper:2181 ls /brokers/ids
    Risponde:

    Connecting to zookeeper:2181

    WATCHER::

    WatchedEvent state:SyncConnected type:None path:null
    [1]


******************************** COMANDI DA RIGA DI COMANDO **********************************

Entriamo nel container del broker
    docker exec -it broker bash 

Creiamo il topic testing
    kafka-topics --bootstrap-server broker:9092 --create --partitions 1 --replication-factor 1 --topic testing 

Eliminiamo il topic testing
    kafka-topics --bootstrap-server localhost:9092 --topic testing --delete    

Per vedere i dettagli del topic possiamo usare:
    kafka-topics --bootstrap-server kafka:9092 --describe --topic testing

Per vedere la lista dei topic
    kafka-topics --bootstrap-server kafka:9092 --list

KAFKA-CONSOLE-PRODUCER
    Entriamo nel container del broker
        docker exec -it broker bash 

    CHIAVI NULLE
        Per scrivere dei dati nel topic con chiavi nulle testing eseguiamo:  
            kafka-console-producer --broker-list broker:9092 --topic testing
    CHIAVI VALORIZZATE
        Per scrivere degli eventi con le chiavi valorizzate usiamo:
            kafka-console-producer --broker-list broker:9092 --topic testing --property parse.key=true --property key.separator=,
    

KAFKA-CONSOLE-CONSUMER
    Entriamo nel container del broker
        docker exec -it broker bash    

    Per leggere i dati nel topic testing eseguiamo:
        
        CHIAVI NULLE
            kafka-console-consumer --bootstrap-server broker:9092 --from-beginning --topic testing

            questo consumer resta in ascolto quindi se continuiamo ad inserire dati nel topic il consumer ce li mostrerà

        CHIAVI VALORIZZATE
            kafka-console-consumer --bootstrap-server broker:9092 --from-beginning --topic testing --property print.key=true

******************************** ESEMPIO PRODUCER **********************************
    Entriamo nel container del broker
        docker exec -it broker bash 

    Creo il topic 
        kafka-topics --create --bootstrap-server broker:9092 --partitions 6 --replication-factor 1 --topic vehicle-positions


        docker image build -t rosariob/producer:v2 .    


        docker run  --rm -d --network confluent_kafka --name producer rosariob/producer:v2



********************************* COME FARE DEBUG SU KAFKA *****************************
https://www.baeldung.com/kafka-docker-connection

Per provare se riesco a scrivere su un topic sul nostro broker presente su un container docker possiamo fare così:

        docker exec -it broker bash   
        
        kafka-console-producer --bootstrap-server broker:29092 --topic vehicle-positions

Per leggere il contenuto di un topic sul nostro broker presente su un container docker possiamo fare così:

        docker exec -it broker bash

        kafka-console-consumer --bootstrap-server broker:29092 --from-beginning --topic vehicle-positions-oper-47 --property print.key=true

Nota: nel parametro --bootstrap-server ho messo broker:29092 perché nel broker ho settato KAFKA_ADVERTISED_LISTENERS PLAINTEXT://broker:29092, PLAINTEXT_HOST://localhost:9092


********************************** MAVEN WRAPPER *****************************************
https://www.baeldung.com/maven-wrapper

Come creare i files wrapper mvnw, mvnw.cmd e il folder .mvn

mvn -N wrapper:wrapper


****************************************************** COMANDI KSQL CLI ******************************************************
Per lanciare questi comandi bisogna prima entrare nel container contenente il server KSQL. 
    Es: docker exec -it ksql-server bash

e successivamente eseguire:
    ksql http://ksql-server:8088

***************** CREATE STREAM *****************
    CREATE STREAM pageviews_original (
            viewtime bigint,
            userid varchar,
            pageid varchar
        ) WITH (kafka_topic='pageviews', value_format='DELIMITED');

    Crea uno stream dal topic sottostante pageviews (Il topic dev'essere già presente su kafka). 

    Si dice che lo stream è backed by the topic

    NOTA:
    Questa istruzione è diversa dall'uso di CREATE STREAM AS SELECT perché quest'ultimo crea uno stream a partire da una query SELECT.

****** DESCRIBE **************

    DESCRIBE pageviews_original; ci dà una descrizione dello stream/table 


***************** CREATE TABLE *****************
    CREATE TABLE users_original (
      registertime BIGINT,
      gender VARCHAR,
      regionid VARCHAR,
      userid VARCHAR
    ) WITH
    (kafka_topic='users', value_format='JSON', key = 'userid');

    Creiamo una table dal topic users


***************** SHOW STREAMS/TABLES *****************
    SHOW STREAMS; ci restituisce la lista degli streams
    
    SHOW TABLES; ci restituisce la lista delle tables

***************** SELECT *****************
    SELECT * FROM pageviews_original LIMIT 10; --> select sullo stream

    SELECT * FROM pageviews_original; --> questa query è infinita e per bloccarla serve ctrl + c.

    SELECT * FROM users_original LIMIT 5; --> select sulla table

    SELECT * FROM users_original; --> anche questa gira all'infinito e per uscire serve ctrl + c

***************** WHERE *****************

    SELECT * FROM pageviews_original WHERE userid='User_1' LIMIT 5;

    SELECT * FROM pageviews_original WHERE pageid LIKE 'Page_6%' LIMIT 5; 

    SELECT pageid, userid FROM pageviews_original LIMIT 5;

***************** JOIN di uno Stream con una Table *****************
    SELECT users_original.userid AS userid, pageid, regionid, gender
    FROM pageviews_original LEFT JOIN users_original
    ON pageviews_original.userid = users_original.userid
     LIMIT 10;

    Arricchiamo lo stream pageviews_original con dei dati dalla tabella users_original

    SELECT s.country, s.name, t.temp
    FROM mytemperatures t
    LEFT JOIN weather_stations s ON t.station_id = s.id;

***************** Creare Stream derivati *****************
    Delle volte vogliamo creare streams o tables che derivano da stream e tables esistenti:

    CREATE STREAM pageviews_female_1 AS
    SELECT users_original.userid AS userid, pageid, regionid, gender
    FROM pageviews_original LEFT JOIN users_original
    ON pageviews_original.userid = users_original.userid
    WHERE gender='FEMALE' AND regionid='Region_1';

    Creiamo un nuovo Stream pageviews_female_1 da uno Stream già presente pageviews_original

    Ovviamente su questo nuovo stream possiamo interrogarlo con le classiche SELECT, WHERE ecc..


 ************* GROUP BY ********************

    SELECT station_id, MAX(temp) AS tmax
    FROM mytemperatures
    GROUP BY station_id;   


************** HAVING ***********************
    SELECT station_id, COUNT( * ) AS nbr
    FROM mytemperatures
    GROUP BY station_id
    HAVING COUNT( * )>=2;    

**************** SET per Impostare proprietà sugli Streams *******************
Una volta entrati nella CLI del server KSQL, quindi:

     ksql http://ksql-server:8088

Potremmo ad esempio volere impostare che tutti gli Streams siano letti dall'inizio. Per fare ciò usiamo SET     

    SET 'auto.offset.reset' = 'earliest';

***************** PRINT serve a vedere il contenuto di un topic in KSQL **************

    PRINT 'stations' FROM BEGINNING;
